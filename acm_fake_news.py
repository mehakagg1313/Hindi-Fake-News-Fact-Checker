# -*- coding: utf-8 -*-
"""acm_fake_news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nc27xKg94jzB5awrl7z8tn6Q9CEyl9mv
"""

import pandas as pd

from google.colab import drive
drive.mount("/content/gdrive")

!pip install inltk
from inltk.inltk import setup
#setup('hi')

from inltk.inltk import tokenize

from google.colab import files
uploaded = files.upload()

df=pd.read_csv("demo.csv")

df.columns =['heading', 'text', 'link', 'emp','output']
df=df.drop(['heading','link','emp'], axis = 1)

import nltk
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import math
df['heading'] = df['heading'].apply(lambda x:tokenize(x,"hi"))
df['text'] = df['text'].apply(lambda x:tokenize(x,"hi"))
df['link'] = df['link'].apply(lambda x:tokenize(x,"hi"))

import regex 
from sklearn.feature_extraction.text import TfidfVectorizer
def custom_analyzer(text):
    words = regex.findall(r'\w{2,}', text) # extract words of at least 2 letters
    for w in words:
        yield w

test = []
#test.append("हमें फिल्म बहुत अच्छी लगी ।")
#test.append("फिल्म में कुछ बेहतरीन गाने हैं ।")
count_vect = TfidfVectorizer(analyzer = custom_analyzer)
#df['heading'] = df['heading'].apply(lambda x:count_vect.fit_transform([x]))
x_train=count_vect.fit_transform(x_train)

x_test=count_vect.transform([x_test])
#x_train['text'] = x_train['text'].apply(lambda x:count_vect.fit_transform([x]))

#x_test['text'] = x_test['text'].apply(lambda x:count_vect.transform([x]))
#x_train['link'] = x_train['link'].apply(lambda x:count_vect.fit_transform([x]))
#xv = count_vect.fit_transform(df['heading'])
#count_vect.get_feature_names()
#df

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report

df=df.drop(['heading','link'],axis=1)

lr = LogisticRegression()
#Makes multiclassifier to binary
ovr=OneVsRestClassifier(lr)
from sklearn.model_selection import train_test_split
x_train, x_test = train_test_split(df, test_size=0.3)

x_tr=[]
for i in x_train['text']:
  x_tr.append(i)
x_ts=[]
for i in x_test['text']:
  x_ts.append(i)
x_ts
y_train=x_train['output']

y_test=x_test['output']

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer=TfidfVectorizer()
#learn vocabulary and return document-term matrix
x_train=vectorizer.fit_transform(x_tr)
#transform document to document term matrix
x_test=vectorizer.transform(x_ts)

y_train=x_train['output']
x_train=x_train.drop(['output'],axis=1)
y_test=x_test['output']
x_test=x_test.drop(['output'],axis=1)

import numpy as np
y_train=np.array(y_train)
y_train
x_train=np.array(x_train)

ovr.fit(x_train,y_train)

y_pred=ovr.predict(x_test)

print("Logistic Regression Result word on Train")
print(classification_report( ovr.predict(x_train) , y_train))

!pip install cltk
from cltk.tokenize.sentence import TokenizeSentence